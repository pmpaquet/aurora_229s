{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "html[data-theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1f1f1f;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "  height: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: \"►\";\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: \"▼\";\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: \"(\";\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: \")\";\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: \",\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 1GB\n",
       "Dimensions:              (time: 4, level: 13, latitude: 721, longitude: 1440)\n",
       "Coordinates:\n",
       "  * latitude             (latitude) float32 3kB -90.0 -89.75 ... 89.75 90.0\n",
       "  * level                (level) int32 52B 50 100 150 200 ... 700 850 925 1000\n",
       "  * longitude            (longitude) float32 6kB 0.0 0.25 0.5 ... 359.5 359.8\n",
       "  * time                 (time) datetime64[ns] 32B 2022-02-01 ... 2022-02-01T...\n",
       "Data variables:\n",
       "    temperature          (time, level, latitude, longitude) float32 216MB ...\n",
       "    u_component_of_wind  (time, level, latitude, longitude) float32 216MB ...\n",
       "    v_component_of_wind  (time, level, latitude, longitude) float32 216MB ...\n",
       "    specific_humidity    (time, level, latitude, longitude) float32 216MB ...\n",
       "    geopotential         (time, level, latitude, longitude) float32 216MB ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-f227655c-fc17-497d-b579-47852f7b0e90' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-f227655c-fc17-497d-b579-47852f7b0e90' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 4</li><li><span class='xr-has-index'>level</span>: 13</li><li><span class='xr-has-index'>latitude</span>: 721</li><li><span class='xr-has-index'>longitude</span>: 1440</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-293c8839-f9c2-40fa-b552-3230e4d5d96f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-293c8839-f9c2-40fa-b552-3230e4d5d96f' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-90.0 -89.75 -89.5 ... 89.75 90.0</div><input id='attrs-332e5453-0aed-405b-aa9c-655888fca087' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-332e5453-0aed-405b-aa9c-655888fca087' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-96eb6533-7006-499e-ae95-41e472241d8f' class='xr-var-data-in' type='checkbox'><label for='data-96eb6533-7006-499e-ae95-41e472241d8f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-90.  , -89.75, -89.5 , ...,  89.5 ,  89.75,  90.  ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>level</span></div><div class='xr-var-dims'>(level)</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>50 100 150 200 ... 700 850 925 1000</div><input id='attrs-59f8be4d-4573-4b6a-a6f5-0db9b12b446d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-59f8be4d-4573-4b6a-a6f5-0db9b12b446d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-08e19a5d-b4b9-4ffb-b782-fc78d3528149' class='xr-var-data-in' type='checkbox'><label for='data-08e19a5d-b4b9-4ffb-b782-fc78d3528149' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([  50,  100,  150,  200,  250,  300,  400,  500,  600,  700,  850,  925,\n",
       "       1000], dtype=int32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0 0.25 0.5 ... 359.2 359.5 359.8</div><input id='attrs-a2ec295e-2f5e-4e9d-8ecc-f8f434b172c8' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a2ec295e-2f5e-4e9d-8ecc-f8f434b172c8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f09cfb68-52f5-4ac8-a295-b1a3bafb9bfe' class='xr-var-data-in' type='checkbox'><label for='data-f09cfb68-52f5-4ac8-a295-b1a3bafb9bfe' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n",
       "       3.5975e+02], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2022-02-01 ... 2022-02-01T18:00:00</div><input id='attrs-02bbd1b5-e0dc-4a96-bd2d-01c9314d16c4' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-02bbd1b5-e0dc-4a96-bd2d-01c9314d16c4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-35b26b1c-0ceb-473e-9a4e-c87d6bacbff0' class='xr-var-data-in' type='checkbox'><label for='data-35b26b1c-0ceb-473e-9a4e-c87d6bacbff0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>initial time of forecast</dd><dt><span>standard_name :</span></dt><dd>forecast_reference_time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2022-02-01T00:00:00.000000000&#x27;, &#x27;2022-02-01T06:00:00.000000000&#x27;,\n",
       "       &#x27;2022-02-01T12:00:00.000000000&#x27;, &#x27;2022-02-01T18:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-86e793ec-688c-428f-adba-03d4720836bf' class='xr-section-summary-in' type='checkbox'  checked><label for='section-86e793ec-688c-428f-adba-03d4720836bf' class='xr-section-summary' >Data variables: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>temperature</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0f1d2ca8-b95c-43d2-b56b-5220a529ea35' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-0f1d2ca8-b95c-43d2-b56b-5220a529ea35' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bfbb7111-7f06-4aae-b17b-94ae393eafa4' class='xr-var-data-in' type='checkbox'><label for='data-bfbb7111-7f06-4aae-b17b-94ae393eafa4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Temperature</dd><dt><span>short_name :</span></dt><dd>t</dd><dt><span>standard_name :</span></dt><dd>air_temperature</dd><dt><span>units :</span></dt><dd>K</dd></dl></div><div class='xr-var-data'><pre>[53988480 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>u_component_of_wind</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-188d5e20-773c-4a1d-9c28-caed31d94207' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-188d5e20-773c-4a1d-9c28-caed31d94207' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b1a75126-2db0-4e32-a0d1-be6fc7964c25' class='xr-var-data-in' type='checkbox'><label for='data-b1a75126-2db0-4e32-a0d1-be6fc7964c25' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>U component of wind</dd><dt><span>short_name :</span></dt><dd>u</dd><dt><span>standard_name :</span></dt><dd>eastward_wind</dd><dt><span>units :</span></dt><dd>m s**-1</dd></dl></div><div class='xr-var-data'><pre>[53988480 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>v_component_of_wind</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-f8c0aaab-51ce-4632-a0a5-62aa09326c5d' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f8c0aaab-51ce-4632-a0a5-62aa09326c5d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c9c8c34a-1908-4c13-833b-87eaf8ed80a8' class='xr-var-data-in' type='checkbox'><label for='data-c9c8c34a-1908-4c13-833b-87eaf8ed80a8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>V component of wind</dd><dt><span>short_name :</span></dt><dd>v</dd><dt><span>standard_name :</span></dt><dd>northward_wind</dd><dt><span>units :</span></dt><dd>m s**-1</dd></dl></div><div class='xr-var-data'><pre>[53988480 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>specific_humidity</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8b8a1588-70b5-4307-b80e-32e1ee2fd0a0' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8b8a1588-70b5-4307-b80e-32e1ee2fd0a0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e52fb417-9d2e-444f-94a1-0d4b0fc776d9' class='xr-var-data-in' type='checkbox'><label for='data-e52fb417-9d2e-444f-94a1-0d4b0fc776d9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Specific humidity</dd><dt><span>short_name :</span></dt><dd>q</dd><dt><span>standard_name :</span></dt><dd>specific_humidity</dd><dt><span>units :</span></dt><dd>kg kg**-1</dd></dl></div><div class='xr-var-data'><pre>[53988480 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>geopotential</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-26511107-e415-4b2d-95da-6e1e9a56c5be' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-26511107-e415-4b2d-95da-6e1e9a56c5be' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-779063e4-1ae9-49c6-8ebd-4e36f5d7c32c' class='xr-var-data-in' type='checkbox'><label for='data-779063e4-1ae9-49c6-8ebd-4e36f5d7c32c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Geopotential</dd><dt><span>short_name :</span></dt><dd>z</dd><dt><span>standard_name :</span></dt><dd>geopotential</dd><dt><span>units :</span></dt><dd>m**2 s**-2</dd></dl></div><div class='xr-var-data'><pre>[53988480 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-82a15901-b889-4c8a-ac94-cce91906f8fc' class='xr-section-summary-in' type='checkbox'  ><label for='section-82a15901-b889-4c8a-ac94-cce91906f8fc' class='xr-section-summary' >Indexes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-179e08c5-f5e4-46fd-82ba-27182263e3d3' class='xr-index-data-in' type='checkbox'/><label for='index-179e08c5-f5e4-46fd-82ba-27182263e3d3' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ -90.0, -89.75,  -89.5, -89.25,  -89.0, -88.75,  -88.5, -88.25,  -88.0,\n",
       "       -87.75,\n",
       "       ...\n",
       "        87.75,   88.0,  88.25,   88.5,  88.75,   89.0,  89.25,   89.5,  89.75,\n",
       "         90.0],\n",
       "      dtype=&#x27;float32&#x27;, name=&#x27;latitude&#x27;, length=721))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>level</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-3b329d49-c003-4fdc-a465-2f156428a724' class='xr-index-data-in' type='checkbox'/><label for='index-3b329d49-c003-4fdc-a465-2f156428a724' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000], dtype=&#x27;int32&#x27;, name=&#x27;level&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-34268be7-eddc-4d56-a9ec-0bd4575186d8' class='xr-index-data-in' type='checkbox'/><label for='index-34268be7-eddc-4d56-a9ec-0bd4575186d8' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([   0.0,   0.25,    0.5,   0.75,    1.0,   1.25,    1.5,   1.75,    2.0,\n",
       "         2.25,\n",
       "       ...\n",
       "        357.5, 357.75,  358.0, 358.25,  358.5, 358.75,  359.0, 359.25,  359.5,\n",
       "       359.75],\n",
       "      dtype=&#x27;float32&#x27;, name=&#x27;longitude&#x27;, length=1440))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-0ec99b8b-246b-4cf7-8929-f6fb65e9793a' class='xr-index-data-in' type='checkbox'/><label for='index-0ec99b8b-246b-4cf7-8929-f6fb65e9793a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2022-02-01 00:00:00&#x27;, &#x27;2022-02-01 06:00:00&#x27;,\n",
       "               &#x27;2022-02-01 12:00:00&#x27;, &#x27;2022-02-01 18:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-6a9ac287-703d-48f1-9858-89cfc8f99906' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-6a9ac287-703d-48f1-9858-89cfc8f99906' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 1GB\n",
       "Dimensions:              (time: 4, level: 13, latitude: 721, longitude: 1440)\n",
       "Coordinates:\n",
       "  * latitude             (latitude) float32 3kB -90.0 -89.75 ... 89.75 90.0\n",
       "  * level                (level) int32 52B 50 100 150 200 ... 700 850 925 1000\n",
       "  * longitude            (longitude) float32 6kB 0.0 0.25 0.5 ... 359.5 359.8\n",
       "  * time                 (time) datetime64[ns] 32B 2022-02-01 ... 2022-02-01T...\n",
       "Data variables:\n",
       "    temperature          (time, level, latitude, longitude) float32 216MB ...\n",
       "    u_component_of_wind  (time, level, latitude, longitude) float32 216MB ...\n",
       "    v_component_of_wind  (time, level, latitude, longitude) float32 216MB ...\n",
       "    specific_humidity    (time, level, latitude, longitude) float32 216MB ...\n",
       "    geopotential         (time, level, latitude, longitude) float32 216MB ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_path = Path(\"/Users/pmpaquet/Projects/Stanford/CS229S/Project/Project_aurora/data\")\n",
    "\n",
    "day ='2022-02-01'\n",
    "\n",
    "\n",
    "static_vars_ds = xr.open_dataset(download_path / \"static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(download_path / day / f\"{day}-surface-level.nc\", engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(download_path / day / f\"{day}-atmospheric.nc\", engine=\"netcdf4\")\n",
    "\n",
    "atmos_vars_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "print(x.pop(0))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 01\n",
      "2 02\n",
      "3 03\n",
      "10 10\n",
      "12 12\n"
     ]
    }
   ],
   "source": [
    "for x in [1, 2, 3, 10, 12]:\n",
    "    print(x, f'{x:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Union, Tuple\n",
    "\n",
    "class InferenceBatcher:\n",
    "    def __init__(self, base_date_list: List[str], data_path: Path) -> None:\n",
    "        self.base_date_list = base_date_list[:]\n",
    "        self.day = self.base_date_list.pop(0)\n",
    "        self.data_path = data_path\n",
    "        self.static_vars_ds = xr.open_dataset(data_path / \"static.nc\", engine=\"netcdf4\")\n",
    "        self.surf_vars_ds: xr.Dataset\n",
    "        self.atmos_vars_ds: xr.Dataset\n",
    "        self._load_date_files()\n",
    "\n",
    "        # Variable names\n",
    "        self.surf_vars_names = [\n",
    "            ('2t', '2m_temperature'),\n",
    "            ('10u', '10m_u_component_of_wind'),\n",
    "            ('10v', '10m_v_component_of_wind'),\n",
    "            ('msl', 'mean_sea_level_pressure'),\n",
    "        ]\n",
    "        self.static_vars_names = [\n",
    "            ('z', 'z'),\n",
    "            ('slt', 'slt'),\n",
    "            ('lsm', 'lsm')\n",
    "        ]\n",
    "        self.atmos_vars_names = [\n",
    "            ('t', 'temperature'),\n",
    "            ('u', 'u_component_of_wind'),\n",
    "            ('v', 'v_component_of_wind'),\n",
    "            ('q', 'specific_humidity'),\n",
    "            ('z', 'geopotential')\n",
    "\n",
    "        ]\n",
    "\n",
    "        self.time_idx: int\n",
    "        self.features: Batch\n",
    "        self.labels: Batch\n",
    "        self._set_initial_feature_labels()\n",
    "\n",
    "\n",
    "    def _load_date_files(self) -> None:\n",
    "        self.surf_vars_ds = xr.open_dataset(\n",
    "            download_path / self.day / f\"{self.day}-surface-level.nc\",\n",
    "            engine=\"netcdf4\"\n",
    "        )\n",
    "        self.atmos_vars_ds = xr.open_dataset(\n",
    "            download_path / self.day / f\"{self.day}-atmospheric.nc\",\n",
    "            engine=\"netcdf4\"\n",
    "        )\n",
    "\n",
    "    def _set_initial_feature_labels(self) -> None:\n",
    "        self.time_idx = 0 # Initialized to 0\n",
    "        self.features = self._make_batch()\n",
    "        self.time_idx += 1\n",
    "        self.labels = self._make_batch()\n",
    "        self.time_idx += 1 # Finish at 2 --> this is the next index to pull from\n",
    "\n",
    "    def _increment_day(self) -> None:\n",
    "        # quick and dirty\n",
    "        days_in_month = {2:28} # 2022 not a leap year\n",
    "        for i in [9, 4, 6, 11]:\n",
    "            days_in_month[i] = 30\n",
    "        # all else is 31\n",
    "\n",
    "        y, m, d = [int(x) for x in self.day.split('-')]\n",
    "        d += 1\n",
    "        if d > days_in_month.get(m, 31):\n",
    "            m += 1\n",
    "            d = 1\n",
    "        assert m <= 12, f'Month is greater than 12: {m}'\n",
    "\n",
    "        self.day = f'{y}-{m:02}-{d:02}'\n",
    "\n",
    "    def _update_internal_state(self) -> bool:\n",
    "        # First, check if time_index (i) is valid\n",
    "        if self.time_idx > 3:\n",
    "            # need to reload new date\n",
    "            self._increment_day()\n",
    "\n",
    "            # check whether the directory exists\n",
    "            if (self.data_path / self.day).is_dir():\n",
    "                # If next day directory exists, load from there\n",
    "                self.time_idx = 0\n",
    "                self._load_date_files()\n",
    "            elif len(self.base_date_list):\n",
    "                # If next day not found, need to jump to new base date\n",
    "                self.day = self.base_date_list.pop(0)\n",
    "                self._load_date_files()\n",
    "                # Need to initialize new states for features and labels\n",
    "                self._set_initial_feature_labels()\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _update_features_and_labels(self) -> None:\n",
    "        '''Updates internal state of features and labels'''\n",
    "        # sh = short-hand, lh = long-hand\n",
    "        self.features = Batch(\n",
    "            surf_vars={\n",
    "                sh:torch.concat((self.features.surf_vars[sh][:,[-1]], self.labels.surf_vars[sh][:, [-1]]), dim=1)\n",
    "                for sh,_ in self.surf_vars_names\n",
    "            },\n",
    "            static_vars=self.labels.static_vars,\n",
    "            atmos_vars={\n",
    "                sh:torch.concat((self.features.atmos_vars[sh][:,[-1]], self.labels.atmos_vars[sh][:, [-1]]), dim=1)\n",
    "                for sh,_ in self.atmos_vars_names\n",
    "            },\n",
    "            metadata=self.labels.metadata,\n",
    "        )\n",
    "        self.labels = self._make_batch()\n",
    "\n",
    "    def _make_batch(self) -> Batch:\n",
    "        def _prepare(x: np.ndarray) -> torch.Tensor:\n",
    "            \"\"\"Prepare a variable.\n",
    "\n",
    "            This does the following things:\n",
    "            * Select time indices `i` and `i - 1`.\n",
    "            * Insert an empty batch dimension with `[None]`.\n",
    "            * Flip along the latitude axis to ensure that the latitudes are decreasing.\n",
    "            * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "            * Convert to PyTorch.\n",
    "            \"\"\"\n",
    "            return torch.from_numpy(x[[self.time_idx]][None][..., ::-1, :].copy())\n",
    "\n",
    "        return Batch(\n",
    "            surf_vars={sh:_prepare(surf_vars_ds[lh].values) for sh,lh in self.surf_vars_names},\n",
    "            static_vars={sh: torch.from_numpy(static_vars_ds[lh].values[0]) for sh,lh in self.static_vars_names},\n",
    "            atmos_vars={sh: _prepare(atmos_vars_ds[lh].values) for sh,lh in self.atmos_vars_names},\n",
    "            metadata=Metadata(\n",
    "                # Flip the latitudes! We need to copy because converting to PyTorch, because the\n",
    "                # data must be contiguous.\n",
    "                lat=torch.from_numpy(surf_vars_ds.latitude.values[::-1].copy()),\n",
    "                lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "                # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "                # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "                # one value for every batch element.\n",
    "                time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[self.time_idx],),\n",
    "                atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_batch(self):\n",
    "        is_valid_batch: bool = self._update_internal_state()\n",
    "\n",
    "        if not is_valid_batch:\n",
    "            return None, None\n",
    "        else:\n",
    "            self._update_features_and_labels()\n",
    "            self.time_idx += 1\n",
    "            return self.features, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = InferenceBatcher(base_date_list=[\"2022-04-01\", \"2022-08-01\"], data_path=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-01\n",
      "1 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-01\n",
      "2 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-02\n",
      "3 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-02\n",
      "4 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-02\n",
      "5 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-02\n",
      "6 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-03\n",
      "7 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-03\n",
      "8 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-03\n",
      "9 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-03\n",
      "10 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-04\n",
      "11 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-04\n",
      "12 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-04\n",
      "13 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-04\n",
      "14 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-05\n",
      "15 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-05\n",
      "16 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-05\n",
      "17 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-05\n",
      "18 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-06\n",
      "19 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-06\n",
      "20 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-06\n",
      "21 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-06\n",
      "22 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-07\n",
      "23 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-07\n",
      "24 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-07\n",
      "25 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-07\n",
      "26 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-08\n",
      "27 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-08\n",
      "28 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-08\n",
      "29 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-08\n",
      "30 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-09\n",
      "31 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-09\n",
      "32 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-09\n",
      "33 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-09\n",
      "34 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-10\n",
      "35 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-10\n",
      "36 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-10\n",
      "37 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-10\n",
      "38 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-11\n",
      "39 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-11\n",
      "40 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-11\n",
      "41 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-11\n",
      "42 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-12\n",
      "43 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-12\n",
      "44 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-12\n",
      "45 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-12\n",
      "46 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-13\n",
      "47 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-13\n",
      "48 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-13\n",
      "49 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-13\n",
      "50 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-14\n",
      "51 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-14\n",
      "52 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-14\n",
      "53 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-14\n",
      "54 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-15\n",
      "55 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-15\n",
      "56 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-15\n",
      "57 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-15\n",
      "58 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-16\n",
      "59 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-16\n",
      "60 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-16\n",
      "61 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-16\n",
      "62 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-17\n",
      "63 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-17\n",
      "64 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-17\n",
      "65 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-17\n",
      "66 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-18\n",
      "67 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-18\n",
      "68 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-18\n",
      "69 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-18\n",
      "70 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-19\n",
      "71 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-19\n",
      "72 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-19\n",
      "73 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-19\n",
      "74 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-20\n",
      "75 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-20\n",
      "76 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-20\n",
      "77 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-20\n",
      "78 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-04-21\n",
      "79 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-04-21\n",
      "80 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-04-21\n",
      "81 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-04-21\n",
      "82 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-01\n",
      "83 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-01\n",
      "84 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-02\n",
      "85 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-02\n",
      "86 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-02\n",
      "87 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-02\n",
      "88 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-03\n",
      "89 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-03\n",
      "90 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-03\n",
      "91 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-03\n",
      "92 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-04\n",
      "93 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-04\n",
      "94 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-04\n",
      "95 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-04\n",
      "96 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-05\n",
      "97 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-05\n",
      "98 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-05\n",
      "99 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-05\n",
      "100 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-06\n",
      "101 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-06\n",
      "102 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-06\n",
      "103 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-06\n",
      "104 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-07\n",
      "105 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-07\n",
      "106 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-07\n",
      "107 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-07\n",
      "108 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-08\n",
      "109 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-08\n",
      "110 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-08\n",
      "111 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-08\n",
      "112 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-09\n",
      "113 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-09\n",
      "114 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-09\n",
      "115 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-09\n",
      "116 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-10\n",
      "117 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-10\n",
      "118 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-10\n",
      "119 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-10\n",
      "120 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-11\n",
      "121 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-11\n",
      "122 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-11\n",
      "123 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-11\n",
      "124 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-12\n",
      "125 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-12\n",
      "126 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-12\n",
      "127 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-12\n",
      "128 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-13\n",
      "129 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-13\n",
      "130 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-13\n",
      "131 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-13\n",
      "132 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-14\n",
      "133 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-14\n",
      "134 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-14\n",
      "135 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-14\n",
      "136 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-15\n",
      "137 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-15\n",
      "138 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-15\n",
      "139 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-15\n",
      "140 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-16\n",
      "141 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-16\n",
      "142 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-16\n",
      "143 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-16\n",
      "144 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-17\n",
      "145 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-17\n",
      "146 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-17\n",
      "147 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-17\n",
      "148 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-18\n",
      "149 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-18\n",
      "150 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-18\n",
      "151 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-18\n",
      "152 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-19\n",
      "153 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-19\n",
      "154 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-19\n",
      "155 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-19\n",
      "156 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-20\n",
      "157 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-20\n",
      "158 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-20\n",
      "159 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-20\n",
      "160 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 0 2022-08-21\n",
      "161 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 1 2022-08-21\n",
      "162 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-08-21\n",
      "163 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 3 2022-08-21\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "164 --> NONE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m feats, labels \u001b[38;5;241m=\u001b[39m batcher\u001b[38;5;241m.\u001b[39mget_batch()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --> NONE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, feats\u001b[38;5;241m.\u001b[39msurf_vars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2t\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, labels\u001b[38;5;241m.\u001b[39msurf_vars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2t\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, batcher\u001b[38;5;241m.\u001b[39mtime_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, batcher\u001b[38;5;241m.\u001b[39mday)\n\u001b[1;32m      7\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 164 --> NONE"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    feats, labels = batcher.get_batch()\n",
    "    if feats is None or labels is None:\n",
    "        assert False, f'{i} --> NONE'\n",
    "    print(i, feats.surf_vars['2t'].shape, labels.surf_vars['2t'].shape, batcher.time_idx - 1, batcher.day)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import inference_helper\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(inference_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()\n",
    "batcher = inference_helper.InferenceBatcher(base_date_list=[\"2022-02-01\", \"2022-07-01\"], data_path=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 2, 721, 1440]) torch.Size([1, 1, 721, 1440]) 2 2022-02-01\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    feats, labels = batcher.get_batch()\n",
    "    if feats is None or labels is None:\n",
    "        assert False, f'{i} --> NONE'\n",
    "    print(i, feats.surf_vars['2t'].shape, labels.surf_vars['2t'].shape, batcher.time_idx - 1, batcher.day)\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import Aurora, rollout\n",
    "\n",
    "model = Aurora()\n",
    "# model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-finetuned.ckpt\")\n",
    "model.load_checkpoint_local(\n",
    "    \"/Users/pmpaquet/Projects/Stanford/CS229S/Project/Project_aurora/models/hf_ckpt/aurora-0.25-finetuned.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aurora(\n",
       "  (encoder): Perceiver3DEncoder(\n",
       "    (surf_mlp): MLP(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (surf_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (pos_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (scale_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (lead_time_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (absolute_time_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (atmos_levels_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (surf_token_embeds): LevelPatchEmbed(\n",
       "      (weights): ParameterDict(\n",
       "          (10u): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (10v): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (2t): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (lsm): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (msl): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (slt): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (z): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (atmos_token_embeds): LevelPatchEmbed(\n",
       "      (weights): ParameterDict(\n",
       "          (q): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (t): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (u): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (v): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "          (z): Parameter containing: [torch.FloatTensor of size 512x1x2x4x4]\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (level_agg): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2-3): 2 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (backbone): Swin3DTransformerBackbone(\n",
       "    (time_mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=512, window_size=(2, 6, 12), num_heads=8\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging3D(\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-9): 10 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=1024, window_size=(2, 6, 12), num_heads=16\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging3D(\n",
       "          (reduction): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=2048, window_size=(2, 6, 12), num_heads=32\n",
       "              (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "              (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=2048, window_size=(2, 6, 12), num_heads=32\n",
       "              (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "              (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchSplitting3D(\n",
       "          (lin1): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (lin2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-9): 10 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=1024, window_size=(2, 6, 12), num_heads=16\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchSplitting3D(\n",
       "          (lin1): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (lin2): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=512, window_size=(2, 6, 12), num_heads=8\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (lora_proj): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (lora_qkv): LoRARollout(\n",
       "                (loras): ModuleList(\n",
       "                  (0): LoRA(\n",
       "                    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Perceiver3DDecoder(\n",
       "    (level_decoder): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2-3): 2 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (surf_heads): ParameterDict(\n",
       "        (10u): Object of type: Linear\n",
       "        (10v): Object of type: Linear\n",
       "        (2t): Object of type: Linear\n",
       "        (msl): Object of type: Linear\n",
       "      (10u): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (10v): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (2t): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (msl): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (atmos_heads): ParameterDict(\n",
       "        (q): Object of type: Linear\n",
       "        (t): Object of type: Linear\n",
       "        (u): Object of type: Linear\n",
       "        (v): Object of type: Linear\n",
       "        (z): Object of type: Linear\n",
       "      (q): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (t): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (u): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (v): Linear(in_features=1024, out_features=16, bias=True)\n",
       "      (z): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (atmos_levels_embed): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = model.to(\"mps\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = model.to(\"cpu\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/rollout.py:33\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(model, batch, steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(p\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 33\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pred\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Add the appropriate history so the model can be run on the prediction.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/model/aurora.py:208\u001b[0m, in \u001b[0;36mAurora.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    204\u001b[0m     batch,\n\u001b[1;32m    205\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[0;32m--> 208\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    215\u001b[0m     x,\n\u001b[1;32m    216\u001b[0m     batch,\n\u001b[1;32m    217\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    218\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Remove batch and history dimension from static variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/model/swin3d.py:913\u001b[0m, in \u001b[0;36mSwin3DTransformerBackbone.forward\u001b[0;34m(self, x, lead_time, rollout_step, patch_res)\u001b[0m\n\u001b[1;32m    911\u001b[0m skips \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers):\n\u001b[0;32m--> 913\u001b[0m     x, x_unscaled \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_enc_res\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     skips\u001b[38;5;241m.\u001b[39mappend(x_unscaled)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers):\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/model/swin3d.py:722\u001b[0m, in \u001b[0;36mBasicLayer3D.forward\u001b[0;34m(self, x, c, res, crop, rollout_step)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the basic layer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tokens.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     x_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x, res)\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/model/swin3d.py:486\u001b[0m, in \u001b[0;36mSwin3DTransformerBlock.forward\u001b[0;34m(self, x, c, res, rollout_step, warped)\u001b[0m\n\u001b[1;32m    483\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ws[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m ws[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m ws[\u001b[38;5;241m2\u001b[39m], D)  \u001b[38;5;66;03m# (nW*B, ws*ws, D)\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA. Has shape (nW*B, ws*ws, D).\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Merge the windows into the original input (patch) resolution.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ws[\u001b[38;5;241m0\u001b[39m], ws[\u001b[38;5;241m1\u001b[39m], ws[\u001b[38;5;241m2\u001b[39m], D)  \u001b[38;5;66;03m# (nW*B, Wc, Wh, Ww, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/model/swin3d.py:152\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask, rollout_step)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    138\u001b[0m     mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    139\u001b[0m     rollout_step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the forward pass of the window-based multi-head self-attention layer.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output of shape `(nW*B, N, C)`.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_qkv(x, rollout_step)\n\u001b[1;32m    153\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB N (qkv H D) -> qkv B H N D\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, qkv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    154\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# model = model.to(\"mps\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = [pred.to(\"cpu\") for pred in rollout(model, feats, steps=1)]\n",
    "\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from datetime import timedelta\n",
    "\n",
    "from aurora.model.fourier import lead_time_expansion\n",
    "\n",
    "def custom_model_forward(model: Aurora, batch: Batch) -> Batch:\n",
    "    \"\"\"Forward pass.\n",
    "\n",
    "    Args:\n",
    "        batch (:class:`Batch`): Batch to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        :class:`Batch`: Prediction for the batch.\n",
    "    \"\"\"\n",
    "    # Get the first parameter. We'll derive the data type and device from this parameter.\n",
    "    p = next(model.parameters())\n",
    "    batch = batch.type(p.dtype)\n",
    "    batch = batch.normalise(surf_stats=batch.surf_stats)\n",
    "    batch = batch.crop(patch_size=model.patch_size)\n",
    "    batch = batch.to('mps')\n",
    "\n",
    "    H, W = batch.spatial_shape\n",
    "    patch_res = (\n",
    "        model.encoder.latent_levels,\n",
    "        H // model.encoder.patch_size,\n",
    "        W // model.encoder.patch_size,\n",
    "    )\n",
    "\n",
    "    # Insert batch and history dimension for static variables.\n",
    "    B, T = next(iter(batch.surf_vars.values())).shape[:2]\n",
    "    batch = dataclasses.replace(\n",
    "        batch,\n",
    "        static_vars={k: v[None, None].repeat(B, T, 1, 1) for k, v in batch.static_vars.items()},\n",
    "    )\n",
    "\n",
    "    encoder = model.encoder.to('mps')\n",
    "    with torch.inference_mode():\n",
    "        x = encoder(\n",
    "            batch,\n",
    "            lead_time=timedelta(hours=6),\n",
    "        )\n",
    "    encoder.to('cpu')\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    # Backbone setup\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    lead_time = timedelta(hours=6)\n",
    "    rollout_step = batch.metadata.rollout_step\n",
    "\n",
    "    # Yanked from swin3d forward\n",
    "    _msg = \"Input shape does not match patch size.\"\n",
    "    assert x.shape[1] == patch_res[0] * patch_res[1] * patch_res[2], _msg\n",
    "\n",
    "    # It's costly to pad across the level dimension, so we should not even though our model\n",
    "    # supports it.\n",
    "    _msg = f\"Patch height ({patch_res[0]}) must be divisible by ws[0] ({model.backbone.window_size[0]})\"\n",
    "    assert patch_res[0] % model.backbone.window_size[0] == 0, _msg\n",
    "\n",
    "    all_enc_res, padded_outs = model.backbone.get_encoder_specs(patch_res)\n",
    "    all_enc_res\n",
    "\n",
    "    lead_hours = lead_time / timedelta(hours=1)\n",
    "    lead_times = lead_hours * torch.ones(x.shape[0], dtype=torch.float32, device=x.device)\n",
    "    c = model.backbone.time_mlp(lead_time_expansion(lead_times, model.backbone.embed_dim).to(dtype=x.dtype))\n",
    "\n",
    "    skips = []\n",
    "    for i, layer in enumerate(model.backbone.encoder_layers):\n",
    "        layer = layer.to('mps')\n",
    "        with torch.inference_mode():\n",
    "            x, x_unscaled = layer(x, c, all_enc_res[i], rollout_step=rollout_step)\n",
    "        skips.append(x_unscaled)\n",
    "        layer = layer.layer.to('cpu')\n",
    "\n",
    "    for i, layer in enumerate(model.backbone.decoder_layers):\n",
    "        index = model.backbone.num_decoder_layers - i - 1\n",
    "        layer = layer.to('mps')\n",
    "        with torch.inference_mode():\n",
    "            x, _ = layer(\n",
    "                x,\n",
    "                c,\n",
    "                all_enc_res[index],\n",
    "                padded_outs[index - 1],\n",
    "                rollout_step=rollout_step,\n",
    "            )\n",
    "        layer = layer.to('cpu')\n",
    "\n",
    "        if 0 < i < model.backbone.num_decoder_layers - 1:\n",
    "            # For the intermediate stages, we use additive skip connections.\n",
    "            x = x + skips[index - 1]\n",
    "        elif i == model.backbone.num_decoder_layers - 1:\n",
    "            # For the last stage, we perform concatentation like in Pangu.\n",
    "            x = torch.cat([x, skips[0]], dim=-1)\n",
    "    # return x\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    # END Backbone setup\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    decoder = model.decoder.to('mps')\n",
    "    with torch.inference_mode():\n",
    "        pred = decoder(\n",
    "            x,\n",
    "            batch,\n",
    "            lead_time=timedelta(hours=6),\n",
    "            patch_res=patch_res,\n",
    "        )\n",
    "    decoder = decoder.to('cpu')\n",
    "\n",
    "    # Remove batch and history dimension from static variables.\n",
    "    pred = dataclasses.replace(\n",
    "        pred,\n",
    "        static_vars={k: v[0, 0] for k, v in batch.static_vars.items()},\n",
    "    )\n",
    "\n",
    "    # Insert history dimension in prediction. The time should already be right.\n",
    "    pred = dataclasses.replace(\n",
    "        pred,\n",
    "        surf_vars={k: v[:, None] for k, v in pred.surf_vars.items()},\n",
    "        atmos_vars={k: v[:, None] for k, v in pred.atmos_vars.items()},\n",
    "    )\n",
    "\n",
    "    pred = pred.unnormalise(surf_stats=model.surf_stats)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def custom_rollout(model: Aurora, batch: Batch, steps: int) -> Generator[Batch, None, None]:\n",
    "    \"\"\"Perform a roll-out to make long-term predictions.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`aurora.model.aurora.Aurora`): The model to roll out.\n",
    "        batch (:class:`aurora.batch.Batch`): The batch to start the roll-out from.\n",
    "        steps (int): The number of roll-out steps.\n",
    "\n",
    "    Yields:\n",
    "        :class:`aurora.batch.Batch`: The prediction after every step.\n",
    "    \"\"\"\n",
    "    # We will need to concatenate data, so ensure that everything is already of the right form.\n",
    "    # Use an arbitary parameter of the model to derive the data type and device.\n",
    "    p = next(model.parameters())\n",
    "    batch = batch.type(p.dtype)\n",
    "    batch = batch.crop(model.patch_size)\n",
    "    batch = batch.to('mps')\n",
    "\n",
    "    for _ in range(steps):\n",
    "        pred = custom_model_forward(model, batch)\n",
    "\n",
    "        yield pred\n",
    "\n",
    "        # Add the appropriate history so the model can be run on the prediction.\n",
    "        batch = dataclasses.replace(\n",
    "            pred,\n",
    "            surf_vars={\n",
    "                k: torch.cat([batch.surf_vars[k][:, 1:], v], dim=1)\n",
    "                for k, v in pred.surf_vars.items()\n",
    "            },\n",
    "            atmos_vars={\n",
    "                k: torch.cat([batch.atmos_vars[k][:, 1:], v], dim=1)\n",
    "                for k, v in pred.atmos_vars.items()\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.11 GB, other allocations: 672.00 KB, max allowed: 18.13 GB). Tried to allocate 102.83 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m, in \u001b[0;36mcustom_rollout\u001b[0;34m(model, batch, steps)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtype(p\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     18\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mcrop(model\u001b[38;5;241m.\u001b[39mpatch_size)\n\u001b[0;32m---> 19\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m     22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m custom_model_forward(model, batch)\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/batch.py:185\u001b[0m, in \u001b[0;36mBatch.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Move the batch to another device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/batch.py:173\u001b[0m, in \u001b[0;36mBatch._fmap\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable[[torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch(\n\u001b[1;32m    171\u001b[0m         surf_vars\u001b[38;5;241m=\u001b[39m{k: f(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    172\u001b[0m         static_vars\u001b[38;5;241m=\u001b[39m{k: f(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m--> 173\u001b[0m         atmos_vars\u001b[38;5;241m=\u001b[39m{k: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matmos_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    174\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mMetadata(\n\u001b[1;32m    175\u001b[0m             lat\u001b[38;5;241m=\u001b[39mf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mlat),\n\u001b[1;32m    176\u001b[0m             lon\u001b[38;5;241m=\u001b[39mf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mlon),\n\u001b[1;32m    177\u001b[0m             atmos_levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39matmos_levels,\n\u001b[1;32m    178\u001b[0m             time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mtime,\n\u001b[1;32m    179\u001b[0m             rollout_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mrollout_step,\n\u001b[1;32m    180\u001b[0m         ),\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/Stanford/CS229S/Project/Project_aurora/aurora_229s/aurora/batch.py:185\u001b[0m, in \u001b[0;36mBatch.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Move the batch to another device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 18.11 GB, other allocations: 672.00 KB, max allowed: 18.13 GB). Tried to allocate 102.83 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "preds = [pred.to(\"cpu\") for pred in custom_rollout(model, feats, steps=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1  # Select this time index in the downloaded data.\n",
    "\n",
    "\n",
    "\n",
    "def _prepare(x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Prepare a variable.\n",
    "\n",
    "        This does the following things:\n",
    "        * Select time indices `i` and `i - 1`.\n",
    "        * Insert an empty batch dimension with `[None]`.\n",
    "        * Flip along the latitude axis to ensure that the latitudes are decreasing.\n",
    "        * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "        * Convert to PyTorch.\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(x[[i - 1, i]][None][..., ::-1, :].copy())\n",
    "\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        \"2t\": _prepare(surf_vars_ds[\"2m_temperature\"].values),\n",
    "        \"10u\": _prepare(surf_vars_ds[\"10m_u_component_of_wind\"].values),\n",
    "        \"10v\": _prepare(surf_vars_ds[\"10m_v_component_of_wind\"].values),\n",
    "        \"msl\": _prepare(surf_vars_ds[\"mean_sea_level_pressure\"].values),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time. They\n",
    "        # don't need to be flipped along the latitude dimension, because they are from\n",
    "        # ERA5.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": _prepare(atmos_vars_ds[\"temperature\"].values),\n",
    "        \"u\": _prepare(atmos_vars_ds[\"u_component_of_wind\"].values),\n",
    "        \"v\": _prepare(atmos_vars_ds[\"v_component_of_wind\"].values),\n",
    "        \"q\": _prepare(atmos_vars_ds[\"specific_humidity\"].values),\n",
    "        \"z\": _prepare(atmos_vars_ds[\"geopotential\"].values),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        # Flip the latitudes! We need to copy because converting to PyTorch, because the\n",
    "        # data must be contiguous.\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values[::-1].copy()),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        # time=(surf_vars_ds.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 721, 1440])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.surf_vars['msl'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
