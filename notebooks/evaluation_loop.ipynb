{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops numpy timm==0.6.13 scipy gcsfs cdsapi xarray zarr netcdf4 matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/aurora_229s\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import inference_helper, evaluation_helper, compression\n",
    "from aurora.model import aurora, swin3d\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(inference_helper)\n",
    "    importlib.reload(evaluation_helper)\n",
    "    importlib.reload(compression)\n",
    "    importlib.reload(aurora)\n",
    "    importlib.reload(swin3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem(msg):\n",
    "    print(f'{msg}:')\n",
    "    print(\"\\ttorch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print()\n",
    "\n",
    "def print_timestamp():\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aurora.AuroraSmall()\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-small-pretrained.ckpt\")\n",
    "model.eval()\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path(\"/workspace/data\")\n",
    "\n",
    "save_dir = Path(\"/workspace/results\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "surf_vars_names_wts, atmos_vars_names_wts = inference_helper.get_vars_names_wts()\n",
    "n_multiday_days = 7\n",
    "multiday_starts = ['2022-05-01']#, '2022-08-01']\n",
    "\n",
    "compression_ratios = [0.5]#, 0.25, 0.75]\n",
    "base_grad_dir = Path(\"/workspace/models/fisher\")\n",
    "# lh_task_names = ['multitask'] + [lh for _,lh,_ in surf_vars_names_wts] + [lh for _,lh,_ in atmos_vars_names_wts]\n",
    "sh_exclude = ['msl', 'z', 'q']\n",
    "# lh_task_names = ['multitask_exclude'] + [lh for sh,lh,_ in surf_vars_names_wts if not (sh in sh_exclude)] + [lh for sh,lh,_ in atmos_vars_names_wts if not (sh in sh_exclude)]\n",
    "lh_task_names = ['multitask_exclude', '2t', 't']\n",
    "\n",
    "sameday_starts = []\n",
    "for day in multiday_starts:\n",
    "    sameday_starts.append(day)\n",
    "    for _ in range(n_multiday_days-1):\n",
    "        day = inference_helper.increment_day(day)\n",
    "        sameday_starts.append(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'baseline'\n",
    "total_df = None\n",
    "\n",
    "# Sameday\n",
    "for day in sameday_starts[:1]: # HACK\n",
    "    day_results_df = evaluation_helper.same_day_eval(model=model, day=day, download_path=download_path, device=device)\n",
    "\n",
    "    if total_df is None:\n",
    "        total_df = day_results_df.copy(deep=True)\n",
    "    else:\n",
    "        total_df = pd.concat([total_df, day_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "(save_dir / 'sameday').mkdir(exist_ok=True, parents=True)\n",
    "total_df.to_csv(save_dir / 'sameday' / f'{model_name}.csv', index=False)\n",
    "del day_results_df, total_df, day\n",
    "\n",
    "# Multiday\n",
    "total_df = None\n",
    "for day in multiday_starts[:1]: # HACK\n",
    "    md_results_df = evaluation_helper.multi_day_eval(\n",
    "        model=model, day=day, download_path=download_path,\n",
    "        max_n_days=2, device=device, verbose=True\n",
    "    )\n",
    "\n",
    "    if total_df is None:\n",
    "        total_df = md_results_df.copy(deep=True)\n",
    "    else:\n",
    "        total_df = pd.concat([total_df, md_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "(save_dir / 'multiday').mkdir(exist_ok=True, parents=True)\n",
    "total_df.to_csv(save_dir / 'multiday' / f'{model_name}.csv', index=False)\n",
    "del md_results_df, total_df, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiday\n",
    "total_df = None\n",
    "for day in multiday_starts:\n",
    "    md_results_df = evaluation_helper.multi_day_eval(\n",
    "        model=model, day=day, download_path=download_path,\n",
    "        max_n_days=2, device=device, verbose=True\n",
    "    )\n",
    "\n",
    "    if total_df is None:\n",
    "        total_df = md_results_df.copy(deep=True)\n",
    "    else:\n",
    "        total_df = pd.concat([total_df, md_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "(save_dir / 'multiday').mkdir(exist_ok=True, parents=True)\n",
    "total_df.to_csv(save_dir / 'multiday' / f'{model_name}.csv', index=False)\n",
    "del md_results_df, total_df, day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "day = '2022-02-01'\n",
    "download_path = Path('/workspace/data')\n",
    "\n",
    "static_vars_ds = xr.open_dataset(download_path / \"static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(download_path / day / f\"{day}-surface-level.nc\", engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(download_path / day / f\"{day}-atmospheric.nc\", engine=\"netcdf4\")\n",
    "\n",
    "i = 1  # Select this time index in the downloaded data.\n",
    "\n",
    "def _prepare(x: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Prepare a variable.\n",
    "\n",
    "    This does the following things:\n",
    "    * Select time indices `i` and `i - 1`.\n",
    "    * Insert an empty batch dimension with `[None]`.\n",
    "    * Flip along the latitude axis to ensure that the latitudes are decreasing.\n",
    "    * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "    * Convert to PyTorch.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x[[i - 1, i]][None][..., ::-1, :].copy())\n",
    "\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        \"2t\": _prepare(surf_vars_ds[\"2m_temperature\"].values),\n",
    "        \"10u\": _prepare(surf_vars_ds[\"10m_u_component_of_wind\"].values),\n",
    "        \"10v\": _prepare(surf_vars_ds[\"10m_v_component_of_wind\"].values),\n",
    "        \"msl\": _prepare(surf_vars_ds[\"mean_sea_level_pressure\"].values),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time. They\n",
    "        # don't need to be flipped along the latitude dimension, because they are from\n",
    "        # ERA5.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": _prepare(atmos_vars_ds[\"temperature\"].values),\n",
    "        \"u\": _prepare(atmos_vars_ds[\"u_component_of_wind\"].values),\n",
    "        \"v\": _prepare(atmos_vars_ds[\"v_component_of_wind\"].values),\n",
    "        \"q\": _prepare(atmos_vars_ds[\"specific_humidity\"].values),\n",
    "        \"z\": _prepare(atmos_vars_ds[\"geopotential\"].values),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        # Flip the latitudes! We need to copy because converting to PyTorch, because the\n",
    "        # data must be contiguous.\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values[::-1].copy()),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_c_model(c_model):\n",
    "    for name,param in c_model.backbone.named_parameters():\n",
    "        assert not bool(torch.any(torch.isnan(param))), name\n",
    "\n",
    "    c_model.eval()\n",
    "    c_model = c_model.to(\"cuda\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        preds = c_model.forward(batch)\n",
    "    c_model = c_model.to(\"cpu\")\n",
    "\n",
    "    for sh,v in c_model.surf_vars.items():\n",
    "        assert not bool(torch.any(torch.isnan(v))), sh\n",
    "    for sh,v in c_model.atmos_vars.items():\n",
    "        assert not bool(torch.any(torch.isnan(v))), sh\n",
    "\n",
    "    print('all good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_c_model(compression.svd_only_compression(original_model=model, ratio=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_c_model(compression.fisher_base_compression(original_model=model, ratio=0.5, grad_path=Path(\"/workspace/models/fisher/multitask_exclude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_c_model(compression.fisher_improved_compression(original_model=model, ratio=0.5, grad_path=Path(\"/workspace/models/fisher/multitask_exclude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD compression loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_sameday_inference_loop(c_model, c_model_name):\n",
    "    # Sameday\n",
    "    if (save_dir / 'sameday' / f'{c_model_name}.csv').is_file():\n",
    "        print('Already exists: ', str(save_dir / 'sameday' / f'{c_model_name}.csv'))\n",
    "        return\n",
    "\n",
    "    print('\\t\\tsameday')\n",
    "    for day in sameday_starts:\n",
    "        print(f'\\t\\t\\t{day}')\n",
    "        day_results_df = evaluation_helper.same_day_eval(model=c_model, day=day, download_path=download_path, device=device)\n",
    "\n",
    "        if total_df is None:\n",
    "            total_df = day_results_df.copy(deep=True)\n",
    "        else:\n",
    "            total_df = pd.concat([total_df, day_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    (save_dir / 'sameday').mkdir(exist_ok=True, parents=True)\n",
    "    total_df.to_csv(save_dir / 'sameday' / f'{c_model_name}.csv', index=False)\n",
    "    \n",
    "def comp_multiday_inference_loop(c_model, c_model_name):\n",
    "    # Multiday\n",
    "    if (save_dir / 'multiday' / f'{c_model_name}.csv').is_file():\n",
    "        print('Already exists: ', str(save_dir / 'multiday' / f'{c_model_name}.csv'))\n",
    "\n",
    "    total_df = None\n",
    "    print('\\t\\tmultiday')\n",
    "    for day in multiday_starts:\n",
    "        print(f'\\t\\t\\t{day}')\n",
    "        md_results_df = evaluation_helper.multi_day_eval(\n",
    "            model=c_model, day=day, download_path=download_path,\n",
    "            max_n_days=n_multiday_days, device=device, verbose=False\n",
    "        )\n",
    "\n",
    "        if total_df is None:\n",
    "            total_df = md_results_df.copy(deep=True)\n",
    "        else:\n",
    "            total_df = pd.concat([total_df, md_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    (save_dir / 'multiday').mkdir(exist_ok=True, parents=True)\n",
    "    total_df.to_csv(save_dir / 'multiday' / f'{c_model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sameday_inference_loop(\n",
    "    c_model=model,\n",
    "    c_model_name='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in compression_ratios:\n",
    "    print(ratio)\n",
    "    comp_sameday_inference_loop(\n",
    "        c_model=compression.svd_only_compression(original_model=model, ratio=ratio),\n",
    "        c_model_name=f'svd_{ratio}'\n",
    "    )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fisher\n",
    "for lh in lh_task_names[:2]:\n",
    "    print(lh)\n",
    "    for ratio in compression_ratios:\n",
    "        print('\\t', ratio)\n",
    "        comp_sameday_inference_loop(\n",
    "            c_model=compression.fisher_base_compression(original_model=model, ratio=ratio, grad_path=base_grad_dir / lh),\n",
    "            c_model_name=f'fisher_base_{lh}_{ratio}'\n",
    "        )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Fisher\n",
    "for lh in lh_task_names[:2]:\n",
    "    print(lh)\n",
    "    for ratio in compression_ratios:\n",
    "        print('\\t', ratio)\n",
    "        comp_sameday_inference_loop(\n",
    "            c_model=compression.fisher_improved_compression(original_model=model, ratio=ratio, grad_path=base_grad_dir / lh),\n",
    "            c_model_name=f'fisher_base_{lh}_{ratio}'\n",
    "        )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
