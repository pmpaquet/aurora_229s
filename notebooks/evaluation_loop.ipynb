{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops numpy timm==0.6.13 scipy gcsfs cdsapi xarray zarr netcdf4 matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/aurora_229s\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import inference_helper, evaluation_helper, compression\n",
    "from aurora.model import aurora, swin3d\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(inference_helper)\n",
    "    importlib.reload(evaluation_helper)\n",
    "    importlib.reload(compression)\n",
    "    importlib.reload(aurora)\n",
    "    importlib.reload(swin3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem(msg):\n",
    "    print(f'{msg}:')\n",
    "    print(\"\\ttorch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print()\n",
    "\n",
    "def print_timestamp():\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aurora.Aurora()\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-small-pretrained.ckpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path(\"/workspace/data\")\n",
    "\n",
    "save_dir = Path(\"/workspace/results\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "surf_vars_names_wts, atmos_vars_names_wts = inference_helper.get_vars_names_wts()\n",
    "n_multiday_days = 7\n",
    "multiday_starts = ['2022-05-01', '2022-08-01']\n",
    "\n",
    "compression_ratios = [0.5, 0.25, 0.75]\n",
    "base_grad_dir = Path(\"/workspace/models/fisher\")\n",
    "lh_task_names = ['multitask'] + [lh for _,lh,_ in surf_vars_names_wts] + [lh for _,lh,_ in atmos_vars_names_wts]\n",
    "\n",
    "sameday_starts = []\n",
    "for day in multiday_starts:\n",
    "    sameday_starts.append(day)\n",
    "    for _ in range(n_multiday_days-1):\n",
    "        day = inference_helper.increment_day(day)\n",
    "        sameday_starts.append(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'baseline'\n",
    "total_df = None\n",
    "\n",
    "# Sameday\n",
    "for day in sameday_starts:\n",
    "    day_results_df = evaluation_helper.same_day_eval(model=model, day=day, download_path=download_path, device=device)\n",
    "\n",
    "    if total_df is None:\n",
    "        total_df = day_results_df.copy(deep=True)\n",
    "    else:\n",
    "        total_df = pd.concat([total_df, day_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "total_df.to_csv(save_dir / 'sameday' / f'{model_name}.csv', index=False)\n",
    "del day_results_df, total_df, day\n",
    "\n",
    "# Multiday\n",
    "total_df = None\n",
    "for day in multiday_starts:\n",
    "    md_results_df = evaluation_helper.multi_day_eval(\n",
    "        model=model, day=day, download_path=download_path,\n",
    "        max_n_days=n_multiday_days, device=device, verbose=True\n",
    "    )\n",
    "\n",
    "    if total_df is None:\n",
    "        total_df = md_results_df.copy(deep=True)\n",
    "    else:\n",
    "        total_df = pd.concat([total_df, md_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "total_df.to_csv(save_dir / 'multiday' / f'{model_name}.csv', index=False)\n",
    "del md_results_df, total_df, day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD compression loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_inference_loop(c_model, c_model_name):\n",
    "    # Sameday\n",
    "    for day in sameday_starts:\n",
    "        day_results_df = evaluation_helper.same_day_eval(model=c_model, day=day, download_path=download_path, device=device)\n",
    "\n",
    "        if total_df is None:\n",
    "            total_df = day_results_df.copy(deep=True)\n",
    "        else:\n",
    "            total_df = pd.concat([total_df, day_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    total_df.to_csv(save_dir / 'sameday' / f'{c_model_name}.csv', index=False)\n",
    "\n",
    "    # Multiday\n",
    "    total_df = None\n",
    "    for day in multiday_starts:\n",
    "        md_results_df = evaluation_helper.multi_day_eval(\n",
    "            model=c_model, day=day, download_path=download_path,\n",
    "            max_n_days=n_multiday_days, device=device, verbose=False\n",
    "        )\n",
    "\n",
    "        if total_df is None:\n",
    "            total_df = md_results_df.copy(deep=True)\n",
    "        else:\n",
    "            total_df = pd.concat([total_df, md_results_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    total_df.to_csv(save_dir / 'multiday' / f'{c_model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in compression_ratios:\n",
    "    print(ratio)\n",
    "    comp_inference_loop(\n",
    "        c_model=compression.svd_only_compression(original_model=model, ratio=ratio),\n",
    "        c_model_name=f'svd_{ratio}'\n",
    "    )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fisher\n",
    "for lh in lh_task_names:\n",
    "    print(lh)\n",
    "    for ratio in compression_ratios:\n",
    "        print('\\t', ratio)\n",
    "        comp_inference_loop(\n",
    "            c_model=compression.fisher_base_compression(original_model=model, ratio=ratio, grad_path=base_grad_dir / lh),\n",
    "            c_model_name=f'fisher_base_{lh}_{ratio}'\n",
    "        )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Fisher\n",
    "for lh in lh_task_names:\n",
    "    print(lh)\n",
    "    for ratio in compression_ratios:\n",
    "        print('\\t', ratio)\n",
    "        comp_inference_loop(\n",
    "            c_model=compression.fisher_improved_compression(original_model=model, ratio=ratio, grad_path=base_grad_dir / lh),\n",
    "            c_model_name=f'fisher_base_{lh}_{ratio}'\n",
    "        )\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
