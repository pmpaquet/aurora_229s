{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops numpy timm==0.6.13 scipy gcsfs cdsapi xarray zarr netcdf4 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/aurora_229s\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import inference_helper\n",
    "from aurora.model import aurora, swin3d\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(inference_helper)\n",
    "    importlib.reload(aurora)\n",
    "    importlib.reload(swin3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem(msg):\n",
    "    print(f'{msg}:')\n",
    "    print(\"\\ttorch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print()\n",
    "\n",
    "def print_timestamp():\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "surf_vars_names = [\n",
    "    ('2t', '2m_temperature'),\n",
    "    ('10u', '10m_u_component_of_wind'),\n",
    "    ('10v', '10m_v_component_of_wind'),\n",
    "    ('msl', 'mean_sea_level_pressure'),\n",
    "]\n",
    "static_vars_names = [\n",
    "    ('z', 'z'),\n",
    "    ('slt', 'slt'),\n",
    "    ('lsm', 'lsm')\n",
    "]\n",
    "atmos_vars_names = [\n",
    "    ('t', 'temperature'),\n",
    "    ('u', 'u_component_of_wind'),\n",
    "    ('v', 'v_component_of_wind'),\n",
    "    ('q', 'specific_humidity'),\n",
    "    ('z', 'geopotential')\n",
    "]\n",
    "\n",
    "all_vars_names = surf_vars_names + atmos_vars_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aurora.AuroraSmall()\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-small-pretrained.ckpt\")\n",
    "# model.load_checkpoint_local(\n",
    "#     \"/workspace/models/hf_ckpt/aurora-0.25-finetuned.ckpt\"\n",
    "# )\n",
    "model.configure_activation_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from aurora.download_data import download_for_day\n",
    "import dataclasses\n",
    "import contextlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import evaluation_helper\n",
    "evaluation_helper.cleanup_download_dir(download_path=Path(\"/workspace/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "\n",
    "# Repeat with multi-task objective\n",
    "surf_vars_names_wts, atmos_vars_names_wts = inference_helper.get_vars_names_wts()\n",
    "\n",
    "# Save May and August for testing\n",
    "base_date_list = [\"2022-02-01\", \"2022-04-01\", \"2022-07-01\", \"2022-11-01\"]\n",
    "# base_date_list = [\"2022-02-01\", \"2022-04-01\"]#, \"2022-07-01\", \"2022-11-01\"]\n",
    "\n",
    "base_save_dir = Path(\"/workspace/models/fisher\")\n",
    "base_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "# device = xm.xla_device()\n",
    "device = 'cuda'\n",
    "\n",
    "sh_exclude = ['msl', 'z', 'q']\n",
    "\n",
    "backbone = deepcopy(model.backbone).to(device)\n",
    "decoder = deepcopy(model.decoder).to(device)\n",
    "# decoder.eval()\n",
    "\n",
    "# MAJOR LOOP -- MTL\n",
    "surf_sh_list = [sh for sh,_,_ in surf_vars_names_wts]\n",
    "all_vars_list = [surf_vars_names_wts[0], atmos_vars_names_wts[0]] + surf_vars_names_wts[1:3] + atmos_vars_names_wts[1:3]\n",
    "for (sh,lh,wt) in all_vars_list:\n",
    "    if sh in sh_exclude:\n",
    "        continue\n",
    "    \n",
    "    cnt = 0\n",
    "    mae_losses = []\n",
    "    # grads = {'backbone_encoder':{}, 'backbone_decoder':{}}\n",
    "    grads = {}\n",
    "    download_for_day(day=base_date_list[0], download_path=Path(\"/workspace/data\"))\n",
    "    batcher = inference_helper.InferenceBatcher(\n",
    "        base_date_list=base_date_list,\n",
    "        data_path=Path(\"/workspace/data\"),\n",
    "        max_n_days=7#was 14\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        # model.zero_grad() # Critical to zero-out gradients\n",
    "        backbone.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        try:\n",
    "            batch, labels = batcher.get_batch()\n",
    "        except Exception as e:\n",
    "            print('\\n', e, '\\n')\n",
    "            break\n",
    "        if batch is None or labels is None:\n",
    "            break\n",
    "        # print(batcher.day, batcher.time_idx - 1)\n",
    "    \n",
    "        rollout_step = batch.metadata.rollout_step\n",
    "        batch = inference_helper.preprocess_batch(model=model, batch=batch, device=device, norm=True)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        p = next(model.parameters())\n",
    "        labels = labels.type(p.dtype)\n",
    "        labels = labels.crop(model.patch_size)\n",
    "        # labels = inference_helper.preprocess_batch(model=model, batch=batch, device=device, norm=False)\n",
    "    \n",
    "        # -------------------------------------\n",
    "        # 2. Encoder Forward\n",
    "        with torch.no_grad():\n",
    "            x, patch_res = inference_helper.encoder_forward(model=model, batch=batch, device=device)\n",
    "        # batch = batch.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # gpu_mem('ENCODER FWD PASS')\n",
    "    \n",
    "        # 3. Backbone encoder layers forward\n",
    "        with torch.no_grad():\n",
    "            c, all_enc_res, padded_outs = inference_helper.backbone_prep(backbone=backbone, x=x, patch_res=patch_res, device=device)\n",
    "            # x, skips, c, all_enc_res, padded_outs = inference_helper.backbone_encoder_layers_forward(\n",
    "            #     model=model, x=x, patch_res=patch_res, rollout_step=rollout_step, device=device,\n",
    "            # )\n",
    "        # print('c', torch.any(torch.isnan(c)))\n",
    "        # print('all_enc_res', torch.any(torch.isnan(torch.tensor(all_enc_res))))\n",
    "        # print('padded_outs', torch.any(torch.isnan(torch.tensor(padded_outs))))\n",
    "        torch.cuda.empty_cache()\n",
    "        # gpu_mem('BACKBONE ENCODER FWD PASS')\n",
    "    \n",
    "        # Step 4?\n",
    "        with torch.autocast(device_type=\"cuda\") if model.autocast else contextlib.nullcontext():\n",
    "            x = backbone(\n",
    "                x,\n",
    "                lead_time=timedelta(hours=6),\n",
    "                patch_res=patch_res,\n",
    "                rollout_step=batch.metadata.rollout_step,\n",
    "            )\n",
    "        # print('after backbone x:', torch.any(torch.isnan(x)))\n",
    "        \n",
    "        # gpu_mem('BDL forward pass')\n",
    "    \n",
    "        # 7. Decoder forward pass\n",
    "        preds = inference_helper.decoder_forward(\n",
    "            decoder=decoder, x=x, batch=batch, patch_res=patch_res, surf_stats=model.surf_stats,\n",
    "        )\n",
    "        del x, batch, patch_res\n",
    "    \n",
    "        # -------------------------------------\n",
    "    \n",
    "        # surf_mae = torch.sum(\n",
    "        #     torch.tensor(\n",
    "        #         [\n",
    "        #             wt*torch.mean(torch.abs(preds.surf_vars[sh_var][0,0] - labels.surf_vars[sh_var][0,0].to(device)))\n",
    "        #             for sh_var,_,wt in surf_vars_names_wts\n",
    "        #         ]\n",
    "        #     )\n",
    "        # )\n",
    "        # atmos_mae = torch.sum(\n",
    "        #     torch.tensor(\n",
    "        #         [\n",
    "        #             wt*torch.mean(torch.abs(preds.atmos_vars[sh_var][0,0] - labels.atmos_vars[sh_var][0,0].to(device)))\n",
    "        #             for sh_var,_,wt in atmos_vars_names_wts\n",
    "        #         ]\n",
    "        #     )\n",
    "        # )\n",
    "        # loss = surf_mae + \n",
    "    \n",
    "        # UGLY UGLY CODE\n",
    "        if sh in surf_sh_list:\n",
    "            loss = wt * torch.mean(torch.abs(preds.surf_vars[sh][0,0] - labels.surf_vars[sh][0,0].to(device)))\n",
    "        else:\n",
    "            loss = wt * torch.mean(torch.abs(preds.atmos_vars[sh][0,0] - labels.atmos_vars[sh][0,0].to(device)))\n",
    "        if cnt < 5:\n",
    "            print(loss)\n",
    "        if bool(torch.any(torch.isnan(loss))):\n",
    "            del loss\n",
    "            continue\n",
    "        loss.backward()\n",
    "        # assert False\n",
    "    \n",
    "        # for name,param in bdl_model.named_parameters():\n",
    "        #     if cnt == 0:\n",
    "        #         grads['backbone_decoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "        #     else:\n",
    "        #         grads['backbone_decoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "    \n",
    "        # for name,param in bel_model.named_parameters():\n",
    "        #     if cnt == 0:\n",
    "        #         grads['backbone_encoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "        #     else:\n",
    "        #         grads['backbone_encoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "        for name,param in backbone.named_parameters():\n",
    "            if cnt == 0:\n",
    "                # print(param.grad.clone().to('cpu').reshape((-1,))[:5])\n",
    "                grads[name] = torch.square(param.grad.clone().to('cpu'))\n",
    "            else:\n",
    "                grads[name] += torch.square(param.grad.clone().to('cpu'))\n",
    "        \n",
    "        cnt += 1\n",
    "        mae_losses.append(loss.clone().detach().to('cpu').numpy())\n",
    "        del preds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # finished with loop\n",
    "    # for key in ['backbone_encoder', 'backbone_decoder']:\n",
    "    #     task_dir = base_save_dir / 'multitask_exclude' / key\n",
    "    #     task_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    #     for name,param in grads[key].items():\n",
    "    #         torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "    task_dir = base_save_dir / lh\n",
    "    task_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for name,param in grads.items():\n",
    "        torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "    np.save(task_dir / f'LOSSES.npy', np.array(mae_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "\n",
    "# Repeat with multi-task objective\n",
    "surf_vars_names_wts, atmos_vars_names_wts = inference_helper.get_vars_names_wts()\n",
    "\n",
    "# Save May and August for testing\n",
    "base_date_list = [\"2022-02-01\", \"2022-04-01\", \"2022-07-01\", \"2022-11-01\"]\n",
    "# base_date_list = [\"2022-02-01\", \"2022-04-01\"]#, \"2022-07-01\", \"2022-11-01\"]\n",
    "\n",
    "base_save_dir = Path(\"/workspace/models/fisher\")\n",
    "base_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "# device = xm.xla_device()\n",
    "device = 'cuda'\n",
    "\n",
    "sh_exclude = ['msl', 'z', 'q']\n",
    "\n",
    "backbone = deepcopy(model.backbone).to(device)\n",
    "decoder = deepcopy(model.decoder).to(device)\n",
    "# decoder.eval()\n",
    "\n",
    "# MAJOR LOOP -- MTL\n",
    "cnt = 0\n",
    "mae_losses = []\n",
    "# grads = {'backbone_encoder':{}, 'backbone_decoder':{}}\n",
    "grads = {}\n",
    "download_for_day(day=base_date_list[0], download_path=Path(\"/workspace/data\"))\n",
    "batcher = inference_helper.InferenceBatcher(\n",
    "    base_date_list=base_date_list,\n",
    "    data_path=Path(\"/workspace/data\"),\n",
    "    max_n_days=7\n",
    ")\n",
    "\n",
    "while True:\n",
    "    # model.zero_grad() # Critical to zero-out gradients\n",
    "    backbone.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    try:\n",
    "        batch, labels = batcher.get_batch()\n",
    "    except Exception as e:\n",
    "        print('\\n', e, '\\n')\n",
    "        break\n",
    "    if batch is None or labels is None:\n",
    "        break\n",
    "    # print(batcher.day, batcher.time_idx - 1)\n",
    "\n",
    "    rollout_step = batch.metadata.rollout_step\n",
    "    batch = inference_helper.preprocess_batch(model=model, batch=batch, device=device, norm=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p = next(model.parameters())\n",
    "    labels = labels.type(p.dtype)\n",
    "    labels = labels.crop(model.patch_size)\n",
    "    # labels = inference_helper.preprocess_batch(model=model, batch=batch, device=device, norm=False)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 2. Encoder Forward\n",
    "    with torch.no_grad():\n",
    "        x, patch_res = inference_helper.encoder_forward(model=model, batch=batch, device=device)\n",
    "    # batch = batch.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    # gpu_mem('ENCODER FWD PASS')\n",
    "\n",
    "    # 3. Backbone encoder layers forward\n",
    "    with torch.no_grad():\n",
    "        c, all_enc_res, padded_outs = inference_helper.backbone_prep(backbone=backbone, x=x, patch_res=patch_res, device=device)\n",
    "        # x, skips, c, all_enc_res, padded_outs = inference_helper.backbone_encoder_layers_forward(\n",
    "        #     model=model, x=x, patch_res=patch_res, rollout_step=rollout_step, device=device,\n",
    "        # )\n",
    "    # print('c', torch.any(torch.isnan(c)))\n",
    "    # print('all_enc_res', torch.any(torch.isnan(torch.tensor(all_enc_res))))\n",
    "    # print('padded_outs', torch.any(torch.isnan(torch.tensor(padded_outs))))\n",
    "    torch.cuda.empty_cache()\n",
    "    # gpu_mem('BACKBONE ENCODER FWD PASS')\n",
    "\n",
    "    # Step 4?\n",
    "    with torch.autocast(device_type=\"cuda\") if model.autocast else contextlib.nullcontext():\n",
    "        x = backbone(\n",
    "            x,\n",
    "            lead_time=timedelta(hours=6),\n",
    "            patch_res=patch_res,\n",
    "            rollout_step=batch.metadata.rollout_step,\n",
    "        )\n",
    "    # print('after backbone x:', torch.any(torch.isnan(x)))\n",
    "    \n",
    "    # gpu_mem('BDL forward pass')\n",
    "\n",
    "    # 7. Decoder forward pass\n",
    "    preds = inference_helper.decoder_forward(\n",
    "        decoder=decoder, x=x, batch=batch, patch_res=patch_res, surf_stats=model.surf_stats,\n",
    "    )\n",
    "    del x, batch, patch_res\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # surf_mae = torch.sum(\n",
    "    #     torch.tensor(\n",
    "    #         [\n",
    "    #             wt*torch.mean(torch.abs(preds.surf_vars[sh_var][0,0] - labels.surf_vars[sh_var][0,0].to(device)))\n",
    "    #             for sh_var,_,wt in surf_vars_names_wts\n",
    "    #         ]\n",
    "    #     )\n",
    "    # )\n",
    "    # atmos_mae = torch.sum(\n",
    "    #     torch.tensor(\n",
    "    #         [\n",
    "    #             wt*torch.mean(torch.abs(preds.atmos_vars[sh_var][0,0] - labels.atmos_vars[sh_var][0,0].to(device)))\n",
    "    #             for sh_var,_,wt in atmos_vars_names_wts\n",
    "    #         ]\n",
    "    #     )\n",
    "    # )\n",
    "    # loss = surf_mae + \n",
    "\n",
    "    # UGLY UGLY CODE\n",
    "    i = 0\n",
    "    loss = surf_vars_names_wts[i][2]*torch.mean(torch.abs(preds.surf_vars[surf_vars_names_wts[i][0]][0,0] - labels.surf_vars[surf_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    for i in range(1,4):\n",
    "        if surf_vars_names_wts[i][0] in sh_exclude:\n",
    "            continue\n",
    "        loss = loss + surf_vars_names_wts[i][2]*torch.mean(torch.abs(preds.surf_vars[surf_vars_names_wts[i][0]][0,0] - labels.surf_vars[surf_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    for i in range(5):\n",
    "        if atmos_vars_names_wts[i][0] in sh_exclude:\n",
    "            continue\n",
    "        loss = loss + atmos_vars_names_wts[i][2]*torch.mean(torch.abs(preds.atmos_vars[atmos_vars_names_wts[i][0]][0,0] - labels.atmos_vars[atmos_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    if cnt < 5:\n",
    "        print(loss)\n",
    "    if bool(torch.any(torch.isnan(loss))):\n",
    "        del loss\n",
    "        continue\n",
    "    \n",
    "    loss.backward()\n",
    "    # assert False\n",
    "\n",
    "    # for name,param in bdl_model.named_parameters():\n",
    "    #     if cnt == 0:\n",
    "    #         grads['backbone_decoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "    #     else:\n",
    "    #         grads['backbone_decoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "\n",
    "    # for name,param in bel_model.named_parameters():\n",
    "    #     if cnt == 0:\n",
    "    #         grads['backbone_encoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "    #     else:\n",
    "    #         grads['backbone_encoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "    for name,param in backbone.named_parameters():\n",
    "        if cnt == 0:\n",
    "            # print(param.grad.clone().to('cpu').reshape((-1,))[:5])\n",
    "            grads[name] = torch.square(param.grad.clone().to('cpu'))\n",
    "        else:\n",
    "            grads[name] += torch.square(param.grad.clone().to('cpu'))\n",
    "    \n",
    "    cnt += 1\n",
    "    mae_losses.append(loss.clone().detach().to('cpu').numpy())\n",
    "    del preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# finished with loop\n",
    "# for key in ['backbone_encoder', 'backbone_decoder']:\n",
    "#     task_dir = base_save_dir / 'multitask_exclude' / key\n",
    "#     task_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for name,param in grads[key].items():\n",
    "#         torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "task_dir = base_save_dir / 'multitask_exclude'\n",
    "task_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name,param in grads.items():\n",
    "    torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "np.save(task_dir / f'LOSSES.npy', np.array(mae_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
