{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops numpy timm==0.6.13 scipy gcsfs cdsapi xarray zarr netcdf4 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/aurora_229s\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import inference_helper\n",
    "from aurora.model import aurora, swin3d\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(inference_helper)\n",
    "    importlib.reload(aurora)\n",
    "    importlib.reload(swin3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem(msg):\n",
    "    print(f'{msg}:')\n",
    "    print(\"\\ttorch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\ttorch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print()\n",
    "\n",
    "def print_timestamp():\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "surf_vars_names = [\n",
    "    ('2t', '2m_temperature'),\n",
    "    ('10u', '10m_u_component_of_wind'),\n",
    "    ('10v', '10m_v_component_of_wind'),\n",
    "    ('msl', 'mean_sea_level_pressure'),\n",
    "]\n",
    "static_vars_names = [\n",
    "    ('z', 'z'),\n",
    "    ('slt', 'slt'),\n",
    "    ('lsm', 'lsm')\n",
    "]\n",
    "atmos_vars_names = [\n",
    "    ('t', 'temperature'),\n",
    "    ('u', 'u_component_of_wind'),\n",
    "    ('v', 'v_component_of_wind'),\n",
    "    ('q', 'specific_humidity'),\n",
    "    ('z', 'geopotential')\n",
    "]\n",
    "\n",
    "all_vars_names = surf_vars_names + atmos_vars_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aurora.Aurora()\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-finetuned.ckpt\")\n",
    "# model.load_checkpoint_local(\n",
    "#     \"/workspace/models/hf_ckpt/aurora-0.25-finetuned.ckpt\"\n",
    "# )\n",
    "model.configure_activation_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from aurora.download_data import download_for_day\n",
    "\n",
    "# Save May and August for testing\n",
    "base_date_list = [\"2022-02-01\", \"2022-04-01\", \"2022-07-01\", \"2022-11-01\"]\n",
    "# base_date_list = [\"2022-02-01\", \"2022-04-01\"]#, \"2022-07-01\", \"2022-11-01\"]\n",
    "\n",
    "base_save_dir = Path(\"/workspace/models/fisher\")\n",
    "base_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "# device = xm.xla_device()\n",
    "device = 'cuda'\n",
    "\n",
    "bel_model = inference_helper.BackboneEncoderLayers(model.backbone.encoder_layers)\n",
    "bel_model = bel_model.to(device)\n",
    "bdl_model = inference_helper.BackboneDecoderLayers(model.backbone.decoder_layers, model.backbone.num_decoder_layers)\n",
    "bdl_model = bdl_model.to(device)\n",
    "decoder = model.decoder.to(device)\n",
    "decoder.eval()\n",
    "\n",
    "# MAJOR LOOP -- SURF VARS\n",
    "# for sh_var,lh_var in all_vars_names:\n",
    "# for sh_var,lh_var in atmos_vars_names:\n",
    "for sh_var,lh_var in (surf_vars_names[2:] + atmos_vars_names[1:]):\n",
    "    print(sh_var, lh_var)\n",
    "    print_timestamp()\n",
    "    print('\\n')\n",
    "\n",
    "    cnt = 0\n",
    "    mae_losses = []\n",
    "    grads = {'backbone_encoder':{}, 'backbone_decoder':{}}\n",
    "    download_for_day(day=base_date_list[0], download_path=Path(\"/workspace/data\"))\n",
    "    batcher = inference_helper.InferenceBatcher(\n",
    "        base_date_list=base_date_list,\n",
    "        data_path=Path(\"/workspace/data\"),\n",
    "        max_n_days=14\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        # model.zero_grad() # Critical to zero-out gradients\n",
    "        bel_model.zero_grad()\n",
    "        bdl_model.zero_grad()\n",
    "        try:\n",
    "            batch, labels = batcher.get_batch()\n",
    "        except Exception as e:\n",
    "            print('\\n', e, '\\n')\n",
    "            break\n",
    "        if batch is None or labels is None:\n",
    "            break\n",
    "        # print(batcher.day, batcher.time_idx - 1)\n",
    "\n",
    "        rollout_step = batch.metadata.rollout_step\n",
    "        batch = inference_helper.preprocess_batch(model=model, batch=batch, device=device)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        p = next(model.parameters())\n",
    "        labels = labels.type(p.dtype)\n",
    "        labels = labels.crop(model.patch_size)\n",
    "\n",
    "        # -------------------------------------\n",
    "        # 2. Encoder Forward\n",
    "        with torch.no_grad():\n",
    "            x, patch_res = inference_helper.encoder_forward(model=model, batch=batch, device=device)\n",
    "        batch = batch.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # gpu_mem('ENCODER FWD PASS')\n",
    "\n",
    "        # 3. Backbone encoder layers forward\n",
    "        with torch.no_grad():\n",
    "            c, all_enc_res, padded_outs = inference_helper.backbone_prep(model=model, x=x, patch_res=patch_res, device=device)\n",
    "            # x, skips, c, all_enc_res, padded_outs = inference_helper.backbone_encoder_layers_forward(\n",
    "            #     model=model, x=x, patch_res=patch_res, rollout_step=rollout_step, device=device,\n",
    "            # )\n",
    "        torch.cuda.empty_cache()\n",
    "        # gpu_mem('BACKBONE ENCODER FWD PASS')\n",
    "\n",
    "        # Step 4?\n",
    "        x, skips = bel_model.forward(x=x, c=c, all_enc_res=all_enc_res, rollout_step=rollout_step)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 6. BDL forward pass\n",
    "        x = bdl_model.forward(\n",
    "            x=x, skips=skips, c=c, all_enc_res=all_enc_res, padded_outs=padded_outs, rollout_step=rollout_step,\n",
    "        )\n",
    "        del skips, c, all_enc_res, padded_outs\n",
    "        torch.cuda.empty_cache()\n",
    "        # gpu_mem('BDL forward pass')\n",
    "\n",
    "        # 7. Decoder forward pass\n",
    "        preds = inference_helper.decoder_forward(\n",
    "            decoder=decoder, x=x, batch=batch, patch_res=patch_res, surf_stats=model.surf_stats,\n",
    "        )\n",
    "        del x, batch, patch_res\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "\n",
    "        if (sh_var,lh_var) in surf_vars_names:\n",
    "            task_pred = preds.surf_vars[sh_var][0, 0]\n",
    "            ref = labels.surf_vars[sh_var][0,0].to(device)\n",
    "        else:\n",
    "            task_pred = preds.atmos_vars[sh_var][0, 0]\n",
    "            ref = labels.atmos_vars[sh_var][0,0].to(device)\n",
    "\n",
    "        # Paper uses mean absolute error\n",
    "        loss = torch.mean(torch.abs(task_pred - ref))\n",
    "        # loss = torch.utils.checkpoint.checkpoint(loss_fn, ref, task_pred, use_reentrant=False)\n",
    "        loss.backward()\n",
    "\n",
    "        for name,param in bdl_model.named_parameters():\n",
    "            if cnt == 0:\n",
    "                grads['backbone_decoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "            else:\n",
    "                grads['backbone_decoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "\n",
    "        for name,param in bel_model.named_parameters():\n",
    "            if cnt == 0:\n",
    "                grads['backbone_encoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "            else:\n",
    "                grads['backbone_encoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "        \n",
    "        cnt += 1\n",
    "        mae_losses.append(loss.clone().detach().to('cpu').numpy())\n",
    "        del preds, task_pred, ref\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # finished with loop\n",
    "    for key in ['backbone_encoder', 'backbone_decoder']:\n",
    "        task_dir = base_save_dir / lh_var / key\n",
    "        task_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        for name,param in grads[key].items():\n",
    "            torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "    np.save(task_dir / f'LOSSES.npy', np.array(mae_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with multi-task objective\n",
    "surf_vars_names_wts = [\n",
    "    ('2t', '2m_temperature', 3.0),\n",
    "    ('10u', '10m_u_component_of_wind', 0.77),\n",
    "    ('10v', '10m_v_component_of_wind', 0.66),\n",
    "    ('msl', 'mean_sea_level_pressure', 1.5),\n",
    "]\n",
    "atmos_vars_names_wts = [\n",
    "    ('t', 'temperature', 1.7),\n",
    "    ('u', 'u_component_of_wind', 0.87),\n",
    "    ('v', 'v_component_of_wind', 0.6),\n",
    "    ('q', 'specific_humidity', 0.78),\n",
    "    ('z', 'geopotential', 2.8)\n",
    "]\n",
    "\n",
    "# Save May and August for testing\n",
    "base_date_list = [\"2022-02-01\", \"2022-04-01\", \"2022-07-01\", \"2022-11-01\"]\n",
    "# base_date_list = [\"2022-02-01\", \"2022-04-01\"]#, \"2022-07-01\", \"2022-11-01\"]\n",
    "\n",
    "base_save_dir = Path(\"/workspace/models/fisher\")\n",
    "base_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "# device = xm.xla_device()\n",
    "device = 'cuda'\n",
    "\n",
    "bel_model = inference_helper.BackboneEncoderLayers(model.backbone.encoder_layers)\n",
    "bel_model = bel_model.to(device)\n",
    "bdl_model = inference_helper.BackboneDecoderLayers(model.backbone.decoder_layers, model.backbone.num_decoder_layers)\n",
    "bdl_model = bdl_model.to(device)\n",
    "decoder = model.decoder.to(device)\n",
    "decoder.eval()\n",
    "\n",
    "# MAJOR LOOP -- MTL\n",
    "cnt = 0\n",
    "mae_losses = []\n",
    "grads = {'backbone_encoder':{}, 'backbone_decoder':{}}\n",
    "download_for_day(day=base_date_list[0], download_path=Path(\"/workspace/data\"))\n",
    "batcher = inference_helper.InferenceBatcher(\n",
    "    base_date_list=base_date_list,\n",
    "    data_path=Path(\"/workspace/data\"),\n",
    "    max_n_days=14\n",
    ")\n",
    "\n",
    "while True:\n",
    "    # model.zero_grad() # Critical to zero-out gradients\n",
    "    bel_model.zero_grad()\n",
    "    bdl_model.zero_grad()\n",
    "    try:\n",
    "        batch, labels = batcher.get_batch()\n",
    "    except Exception as e:\n",
    "        print('\\n', e, '\\n')\n",
    "        break\n",
    "    if batch is None or labels is None:\n",
    "        break\n",
    "    # print(batcher.day, batcher.time_idx - 1)\n",
    "\n",
    "    rollout_step = batch.metadata.rollout_step\n",
    "    batch = inference_helper.preprocess_batch(model=model, batch=batch, device=device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p = next(model.parameters())\n",
    "    labels = labels.type(p.dtype)\n",
    "    labels = labels.crop(model.patch_size)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 2. Encoder Forward\n",
    "    with torch.no_grad():\n",
    "        x, patch_res = inference_helper.encoder_forward(model=model, batch=batch, device=device)\n",
    "    batch = batch.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    # gpu_mem('ENCODER FWD PASS')\n",
    "\n",
    "    # 3. Backbone encoder layers forward\n",
    "    with torch.no_grad():\n",
    "        c, all_enc_res, padded_outs = inference_helper.backbone_prep(model=model, x=x, patch_res=patch_res, device=device)\n",
    "        # x, skips, c, all_enc_res, padded_outs = inference_helper.backbone_encoder_layers_forward(\n",
    "        #     model=model, x=x, patch_res=patch_res, rollout_step=rollout_step, device=device,\n",
    "        # )\n",
    "    torch.cuda.empty_cache()\n",
    "    # gpu_mem('BACKBONE ENCODER FWD PASS')\n",
    "\n",
    "    # Step 4?\n",
    "    x, skips = bel_model.forward(x=x, c=c, all_enc_res=all_enc_res, rollout_step=rollout_step)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 6. BDL forward pass\n",
    "    x = bdl_model.forward(\n",
    "        x=x, skips=skips, c=c, all_enc_res=all_enc_res, padded_outs=padded_outs, rollout_step=rollout_step,\n",
    "    )\n",
    "    del skips, c, all_enc_res, padded_outs\n",
    "    torch.cuda.empty_cache()\n",
    "    # gpu_mem('BDL forward pass')\n",
    "\n",
    "    # 7. Decoder forward pass\n",
    "    preds = inference_helper.decoder_forward(\n",
    "        decoder=decoder, x=x, batch=batch, patch_res=patch_res, surf_stats=model.surf_stats,\n",
    "    )\n",
    "    del x, batch, patch_res\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # surf_mae = torch.sum(\n",
    "    #     torch.tensor(\n",
    "    #         [\n",
    "    #             wt*torch.mean(torch.abs(preds.surf_vars[sh_var][0,0] - labels.surf_vars[sh_var][0,0].to(device)))\n",
    "    #             for sh_var,_,wt in surf_vars_names_wts\n",
    "    #         ]\n",
    "    #     )\n",
    "    # )\n",
    "    # atmos_mae = torch.sum(\n",
    "    #     torch.tensor(\n",
    "    #         [\n",
    "    #             wt*torch.mean(torch.abs(preds.atmos_vars[sh_var][0,0] - labels.atmos_vars[sh_var][0,0].to(device)))\n",
    "    #             for sh_var,_,wt in atmos_vars_names_wts\n",
    "    #         ]\n",
    "    #     )\n",
    "    # )\n",
    "    # loss = surf_mae + \n",
    "\n",
    "    # UGLY UGLY CODE\n",
    "    i = 0\n",
    "    loss = surf_vars_names_wts[i][2]*torch.mean(torch.abs(preds.surf_vars[surf_vars_names_wts[i][0]][0,0] - labels.surf_vars[surf_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    for i in range(1,4):\n",
    "        loss = loss + surf_vars_names_wts[i][2]*torch.mean(torch.abs(preds.surf_vars[surf_vars_names_wts[i][0]][0,0] - labels.surf_vars[surf_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    for i in range(5):\n",
    "        loss = loss + atmos_vars_names_wts[i][2]*torch.mean(torch.abs(preds.atmos_vars[atmos_vars_names_wts[i][0]][0,0] - labels.atmos_vars[atmos_vars_names_wts[i][0]][0,0].to(device)))\n",
    "    loss.backward()\n",
    "\n",
    "    for name,param in bdl_model.named_parameters():\n",
    "        if cnt == 0:\n",
    "            grads['backbone_decoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "        else:\n",
    "            grads['backbone_decoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "\n",
    "    for name,param in bel_model.named_parameters():\n",
    "        if cnt == 0:\n",
    "            grads['backbone_encoder'][name] = torch.square(param.grad.clone().to('cpu'))\n",
    "        else:\n",
    "            grads['backbone_encoder'][name] += torch.square(param.grad.clone().to('cpu'))\n",
    "    \n",
    "    cnt += 1\n",
    "    mae_losses.append(loss.clone().detach().to('cpu').numpy())\n",
    "    del preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# finished with loop\n",
    "for key in ['backbone_encoder', 'backbone_decoder']:\n",
    "    task_dir = base_save_dir / 'multitask' / key\n",
    "    task_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name,param in grads[key].items():\n",
    "        torch.save(param / float(cnt), task_dir / f'{name}.pt')\n",
    "np.save(task_dir.parent / f'LOSSES.npy', np.array(mae_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
